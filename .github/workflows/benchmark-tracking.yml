name: Performance Benchmark Tracking

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  schedule:
    # Run nightly performance benchmarks
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      benchmark_filter:
        description: 'Benchmark filter pattern (optional)'
        required: false
        default: '*'
      dataset_size:
        description: 'Dataset size for benchmarks'
        required: false
        default: '1MB'
        type: choice
        options:
        - '1KB'
        - '1MB'
        - '1GB'

permissions:
  contents: read
  pages: write
  id-token: write

jobs:
  benchmark-baseline:
    name: Benchmark vs Competitors
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        dotnet-version: ['8.0.x', '10.0.x']
      fail-fast: false

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup .NET
      uses: actions/setup-dotnet@v4
      with:
        dotnet-version: ${{ matrix.dotnet-version }}
        include-prerelease: true

    - name: Restore dependencies
      run: dotnet restore

    - name: Build in Release mode
      run: dotnet build --configuration Release --no-restore

    # Generate datasets if they don't exist
    - name: Generate benchmark datasets
      working-directory: tests/HeroParser.BenchmarkTests
      run: |
        if [ ! -d "BenchmarkData" ]; then
          dotnet run --configuration Release -- generate
        fi
      shell: bash

    # Run competitor benchmarks
    - name: Run competitor benchmarks
      working-directory: tests/HeroParser.BenchmarkTests
      run: |
        FILTER="${{ github.event.inputs.benchmark_filter || '*CompetitorBenchmarks*' }}"
        echo "Running benchmarks with filter: $FILTER"

        # Run benchmarks with hardware counters if available
        dotnet run --configuration Release -- \
          --filter "$FILTER" \
          --memory \
          --disasm \
          --runtimes net8.0 \
          --exporters json html csv \
          --artifacts ./BenchmarkDotNet.Artifacts

    # Upload benchmark results
    - name: Upload benchmark artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-results-${{ matrix.os }}-${{ matrix.dotnet-version }}
        path: tests/HeroParser.BenchmarkTests/BenchmarkDotNet.Artifacts/
        retention-days: 30

    # Performance regression detection
    - name: Performance regression check
      run: |
        echo "Checking for performance regressions..."

        # This is a placeholder for performance regression logic
        # In a real implementation, you would:
        # 1. Compare current results with historical baseline
        # 2. Check if performance decreased by more than 2%
        # 3. Fail the build if regression is detected

        RESULTS_FILE="tests/HeroParser.BenchmarkTests/BenchmarkDotNet.Artifacts/results/HeroParser.BenchmarkTests.CompetitorBenchmarks-report.json"

        if [ -f "$RESULTS_FILE" ]; then
          echo "âœ“ Benchmark results generated successfully"

          # Extract key performance metrics (placeholder)
          echo "## Performance Summary" >> $GITHUB_STEP_SUMMARY
          echo "- Platform: ${{ matrix.os }}" >> $GITHUB_STEP_SUMMARY
          echo "- .NET Version: ${{ matrix.dotnet-version }}" >> $GITHUB_STEP_SUMMARY
          echo "- Benchmark completed at: $(date)" >> $GITHUB_STEP_SUMMARY
        else
          echo "âš  Benchmark results not found"
          echo "::warning::Benchmark results file not generated"
        fi

  memory-profiling:
    name: Memory Allocation Profiling
    runs-on: ubuntu-latest
    needs: benchmark-baseline

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup .NET
      uses: actions/setup-dotnet@v4
      with:
        dotnet-version: |
          8.0.x
          10.0.x
        include-prerelease: true

    - name: Restore dependencies
      run: dotnet restore

    - name: Build in Release mode
      run: dotnet build --configuration Release --no-restore

    # Run memory-focused benchmarks
    - name: Memory allocation benchmarks
      working-directory: tests/HeroParser.BenchmarkTests
      run: |
        echo "Running memory allocation benchmarks..."

        # Focus on memory diagnostics
        dotnet run --configuration Release -- \
          --filter "*Memory*" \
          --memory \
          --runtimes net8.0 \
          --exporters json \
          --artifacts ./MemoryBenchmarks

    # Validate zero-allocation requirements
    - name: Zero-allocation validation
      run: |
        echo "Validating zero-allocation requirements..."

        MEMORY_RESULTS="tests/HeroParser.BenchmarkTests/MemoryBenchmarks/results"

        if [ -d "$MEMORY_RESULTS" ]; then
          echo "## Memory Allocation Analysis" >> $GITHUB_STEP_SUMMARY

          # Check for any allocations in core parsing operations
          if find "$MEMORY_RESULTS" -name "*.json" -exec grep -l '"Allocated".*[1-9]' {} \; | head -1; then
            echo "âš  Memory allocations detected in core operations" >> $GITHUB_STEP_SUMMARY
            echo "::warning::Zero-allocation target not met"
          else
            echo "âœ… Zero-allocation target achieved" >> $GITHUB_STEP_SUMMARY
          fi
        fi

    - name: Upload memory profiling results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: memory-profile-results
        path: tests/HeroParser.BenchmarkTests/MemoryBenchmarks/
        retention-days: 7

  performance-comparison:
    name: Performance Comparison Report
    runs-on: ubuntu-latest
    needs: [benchmark-baseline, memory-profiling]
    if: always()

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Download benchmark artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: benchmark-results-*
        path: benchmark-results/
        merge-multiple: true

    - name: Generate performance report
      run: |
        echo "# HeroParser Performance Report" > performance-report.md
        echo "" >> performance-report.md
        echo "**Generated**: $(date)" >> performance-report.md
        echo "**Commit**: ${{ github.sha }}" >> performance-report.md
        echo "" >> performance-report.md

        # Constitution performance targets
        echo "## Performance Targets" >> performance-report.md
        echo "" >> performance-report.md
        echo "| Target | .NET 8 | .NET 10 | Status |" >> performance-report.md
        echo "|--------|--------|---------|--------|" >> performance-report.md
        echo "| Single-threaded | >25 GB/s | >30 GB/s | ðŸ”„ Measuring |" >> performance-report.md
        echo "| Multi-threaded | >50 GB/s | >60 GB/s | ðŸ”„ Measuring |" >> performance-report.md
        echo "| vs Sep (21 GB/s) | >40% faster | >40% faster | ðŸ”„ Measuring |" >> performance-report.md
        echo "| Memory | Zero alloc | Zero alloc | ðŸ”„ Measuring |" >> performance-report.md
        echo "" >> performance-report.md

        # Platform performance summary
        echo "## Platform Performance" >> performance-report.md
        echo "" >> performance-report.md

        for platform in ubuntu-latest windows-latest macos-latest; do
          echo "### $platform" >> performance-report.md
          echo "" >> performance-report.md

          if [ -d "benchmark-results" ]; then
            echo "- Benchmarks completed successfully" >> performance-report.md
          else
            echo "- âš  Benchmark results not available" >> performance-report.md
          fi

          echo "" >> performance-report.md
        done

        # Competitive analysis
        echo "## Competitive Analysis" >> performance-report.md
        echo "" >> performance-report.md
        echo "| Parser | Performance | Memory | Notes |" >> performance-report.md
        echo "|--------|------------|---------|-------|" >> performance-report.md
        echo "| HeroParser | ðŸ”„ TBD | ðŸ”„ TBD | Target implementation |" >> performance-report.md
        echo "| Sep | 21 GB/s | Good | Current leader |" >> performance-report.md
        echo "| Sylvan.Data.Csv | ~18 GB/s | Excellent | Zero-allocation focus |" >> performance-report.md
        echo "| CsvHelper | Baseline | Poor | Feature-rich baseline |" >> performance-report.md

    - name: Add report to job summary
      run: |
        cat performance-report.md >> $GITHUB_STEP_SUMMARY

    - name: Upload performance report
      uses: actions/upload-artifact@v4
      with:
        name: performance-report
        path: performance-report.md
        retention-days: 90

  benchmark-history:
    name: Update Benchmark History
    runs-on: ubuntu-latest
    needs: performance-comparison
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Download benchmark artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: benchmark-results-*
        path: benchmark-history/
        merge-multiple: true

    # Store benchmark history for trending analysis
    - name: Update benchmark history
      run: |
        echo "Updating benchmark history..."

        # Create benchmark history directory if it doesn't exist
        mkdir -p docs/benchmarks/history

        # Archive current benchmark results with timestamp
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        COMMIT_SHORT=$(echo ${{ github.sha }} | cut -c1-8)

        if [ -d "benchmark-history" ]; then
          mv benchmark-history "docs/benchmarks/history/${TIMESTAMP}_${COMMIT_SHORT}"
          echo "âœ“ Archived benchmark results to docs/benchmarks/history/${TIMESTAMP}_${COMMIT_SHORT}"
        fi

        # Generate benchmark trending data (placeholder)
        echo "timestamp,commit,platform,framework,throughput_gbps,allocations" > docs/benchmarks/benchmark-trend.csv
        echo "${TIMESTAMP},${{ github.sha }},ubuntu-latest,net8.0,0,0" >> docs/benchmarks/benchmark-trend.csv

    - name: Commit benchmark history
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"

        git add docs/benchmarks/
        if git diff --staged --quiet; then
          echo "No benchmark history changes to commit"
        else
          git commit -m "ðŸ“Š Update benchmark history for ${{ github.sha }}"
          git push
        fi